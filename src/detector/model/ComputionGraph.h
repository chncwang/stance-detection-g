#ifndef SRC_ComputionGraph_H_
#define SRC_ComputionGraph_H_

#include "ModelParams.h"
#include "ConditionalLSTM.h"
#include "Utf.h"


// Each model consists of two parts, building neural graph and defining output losses.
class GraphBuilder {
public:
    vector<LookupNode> _inputNodes;
    ConditionalLSTMBuilder _left2right;
    ConditionalLSTMBuilder _right2left;
    std::vector<ConcatNode> _tweet_concat_nodes;
    std::vector<ConcatNode> _target_concat_nodes;
    MaxPoolNode _target_pooling;
    UniNode _neural_output;
    AttentionBuilder _attention_builder;

    Graph *_graph;
    ModelParams *_modelParams;
    const static int max_sentence_length = 1024;

public:
    //allocate enough nodes
    void createNodes(int length_upper_bound) {
        _inputNodes.resize(length_upper_bound);
        _left2right.resize(length_upper_bound);
        _right2left.resize(length_upper_bound);
        _tweet_concat_nodes.resize(length_upper_bound);
        _target_concat_nodes.resize(length_upper_bound);
        _target_pooling.setParam(length_upper_bound);
        _attention_builder.resize(length_upper_bound);
    }

public:
    void initial(Graph *pcg, ModelParams &model, HyperParams &opts) {
        _graph = pcg;
        for (LookupNode &n : _inputNodes) {
            n.init(opts.wordDim, opts.dropProb);
            n.setParam(&model.words);
        }
        _left2right.init(opts.dropProb, &model.target_left_to_right_lstm_params, true);
        _right2left.init(opts.dropProb, &model.target_right_to_left_lstm_params, false);

        for (auto &n : _target_concat_nodes) {
            n.init(opts.hiddenSize * 2, -1);
        }

        for (auto &n : _tweet_concat_nodes) {
            n.init(opts.hiddenSize * 2, -1);
        }
        _target_pooling.init(opts.hiddenSize * 2, -1);
        _neural_output.setParam(&model.olayer_linear);
        _neural_output.init(opts.labelSize, -1);
        _attention_builder.init(&model.attention_params);
        _modelParams = &model;
    }

public:
    // some nodes may behave different during training and decode, for example, dropout
    inline void forward(const Feature &feature, bool bTrain = false) {
        _graph->train = bTrain;

        const std::vector<std::string> &target_words = getStanceTargetWords(feature.m_target);

        for (int i = 0; i < target_words.size(); ++i) {
            _inputNodes.at(i).forward(_graph, target_words.at(i));
        }

        for (int i = 0; i < feature.m_tweet_words.size(); ++i) {
            _inputNodes.at(i + target_words.size()).forward(_graph, feature.m_tweet_words.at(i));
        }

        vector<PNode> inputNodes;
        int totalSize = feature.m_tweet_words.size() + target_words.size();
        for (int i = 0; i < totalSize; ++i) {
            inputNodes.push_back(&_inputNodes.at(i));
        }

        _left2right.setParam(&_modelParams->target_left_to_right_lstm_params, &_modelParams->tweet_left_to_right_lstm_params, target_words.size());
        _right2left.setParam(&_modelParams->target_right_to_left_lstm_params, &_modelParams->tweet_right_to_left_lstm_params, target_words.size());

        _left2right.forward(_graph, inputNodes, target_words.size());
        _right2left.forward(_graph, inputNodes, target_words.size());

        for (int i = 0; i < feature.m_tweet_words.size(); ++i) {
            _tweet_concat_nodes.at(i).forward(_graph, &_left2right._hiddens.at(target_words.size() + i),
                &_right2left._hiddens.at(target_words.size() + i));
        }

        for (int i = 0; i < target_words.size(); ++i) {
            _target_concat_nodes.at(i).forward(_graph, &_left2right._hiddens.at(i), &_right2left._hiddens.at(i));
        }

        _target_pooling.forward(_graph, toPointers<ConcatNode, Node>(_target_concat_nodes, target_words.size()));

        _attention_builder.forward(_graph, toPointers<ConcatNode, Node>(_tweet_concat_nodes, feature.m_tweet_words.size()), &_target_pooling);

        _neural_output.forward(_graph, &_attention_builder._hidden);
    }
};


#endif /* SRC_ComputionGraph_H_ */
